

		Understanding Tokenization using Lexical Analysis			-	Tokenization vs. Lexical Analysis

Lexing can be made very easy by simply understanding what the tokenization process is,     The main difference between tokenization and
and what it actually means. Tokenization just refers to turning a set of ASCII		   Lexical Analysis (or Lexer/Lexing for short)
characters into an array of tokens. Let me show you by example.				   is that tokenization is the PROCESS itself
											   of turning ASCII characters into tokens,
The code block:										   and lexing is the name we assigned to that
```adn											   process.
include adan.io;

program::void main() {
	printf("Hello, world!");
}
```

This is what we call SOURCE CODE, otherwise SRC for short. SRC is what the programmer
is typing. After we write our code, we can execute it, which then a compiler will
typically transform that SRC into a set of tokens, like this:

```
[
	INCLUDE, INTIAILIZER, PERIOD, INTIALIZER, SEMICOLON,
	PROGRAM, TYPE_DECLARATOR, TYPE_VOID, INITIALIZER, L_PAREN,
	R_PAREN, L_BRACE, INITIALIZER, L_PAREN, QUOTATION,
	STRING("Hello, world!"), QUOTATION, R_PAREN, SEMICOLON, R_BRACE
]
```

That's realy hard to code in, and it can get very irritating if we tried. But this
stage is very important for the next step, which will turn all of these tokens
into a "tree" of sorts.
